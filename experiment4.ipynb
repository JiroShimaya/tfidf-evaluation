{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datasets\n",
    "import tqdm\n",
    "import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  output/expreiment4/localgovfaq.zip\n",
      "   creating: output/expreiment4/localgovfaq/\n",
      "  inflating: output/expreiment4/localgovfaq/testset_segmentation.txt  \n",
      "  inflating: output/expreiment4/localgovfaq/testset.txt  \n",
      "   creating: output/expreiment4/localgovfaq/qas/\n",
      "  inflating: output/expreiment4/localgovfaq/qas/answers_in_Amagasaki.txt  \n",
      "  inflating: output/expreiment4/localgovfaq/qas/questions_in_Amagasaki.txt  \n",
      "   creating: output/expreiment4/localgovfaq/samples/\n",
      "  inflating: output/expreiment4/localgovfaq/samples/tsubaki.txt  \n",
      "  inflating: output/expreiment4/localgovfaq/samples/bert.txt  \n",
      "  inflating: output/expreiment4/localgovfaq/samples/joint.txt  \n",
      "  inflating: output/expreiment4/localgovfaq/README.md  \n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ku-nlp/bert-based-faqir/tree/master\n",
    "\n",
    "OUTPUT_DIR = \"output/expreiment4\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "!curl https://tulip.kuee.kyoto-u.ac.jp/localgovfaq/localgovfaq.zip > {OUTPUT_DIR}/localgovfaq.zip\n",
    "!unzip {OUTPUT_DIR}/localgovfaq.zip -d {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1786it [00:01, 1108.75it/s]\n",
      "1786it [00:00, 16340.79it/s]\n"
     ]
    }
   ],
   "source": [
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def tokenize_jp(text: str) -> str:\n",
    "    tokens = mecab.parse(text).split()\n",
    "    return tokens\n",
    "\n",
    "# {OUTPUT_DIR}/localgovfaq/qas/answers_in_Amagasaki.txtを読み込んで、各行splitして２要素目以降をjoinし、tokenize_jpで再度分かち書き\n",
    "with open(f\"{OUTPUT_DIR}/localgovfaq/qas/answers_in_Amagasaki.txt\", \"r\") as f:\n",
    "    amagasaki_answers = [tokenize_jp(\"\".join(line.split()[1:])) for line in tqdm.tqdm(f)]\n",
    "# {OUTPUT_DIR}/localgovfaq/qas/questions_in_Amagasaki.txtを読み込んで、各行splitして２要素目以降をjoinし、tokenize_jpで再度分かち書き\n",
    "with open(f\"{OUTPUT_DIR}/localgovfaq/qas/questions_in_Amagasaki.txt\", \"r\") as f:\n",
    "    amagasaki_questions = [tokenize_jp(\"\".join(line.split()[1:])) for line in tqdm.tqdm(f)]\n",
    "\n",
    "amagasaki_vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "amagasaki_vectorizer.fit(amagasaki_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6114221724524076\n"
     ]
    }
   ],
   "source": [
    "def calc_recall(vectorizer, n = 5):\n",
    "    global amagasaki_answers, amagasaki_questions\n",
    "        \n",
    "    answer_matrix = vectorizer.transform(amagasaki_answers)\n",
    "    query_matrix = vectorizer.transform(amagasaki_questions)\n",
    "\n",
    "    # 類似度行列を計算し、queryのdocidのランクを取得\n",
    "    similarity_matrix = cosine_similarity(query_matrix, answer_matrix)\n",
    "    ranking_matrix = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "\n",
    "    # ranking_matrixのi行目で、iが第何位か計算\n",
    "    rank_list = np.array([np.where(ranking_matrix[i] == i)[0][0] for i in range(len(ranking_matrix))])\n",
    "    # recall@nを計算\n",
    "    recall_at_n = np.sum(rank_list < n) / len(rank_list)\n",
    "    return recall_at_n\n",
    "\n",
    "recall_at_5 = calc_recall(amagasaki_vectorizer, n=5)\n",
    "print(recall_at_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 153028/2000000 [02:44<41:38, 739.10it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m full_corpus \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m\"\u001b[39m\u001b[39mmiracl/miracl-corpus\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mja\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m full_corpus_used \u001b[39m=\u001b[39m full_corpus[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mselect(random\u001b[39m.\u001b[39msample(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(full_corpus[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m])), \u001b[39m2000000\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m full_corpus_used \u001b[39m=\u001b[39m [tokenize_jp(item[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m tqdm\u001b[39m.\u001b[39;49mtqdm(full_corpus_used)]\n",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m full_corpus \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m\"\u001b[39m\u001b[39mmiracl/miracl-corpus\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mja\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m full_corpus_used \u001b[39m=\u001b[39m full_corpus[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mselect(random\u001b[39m.\u001b[39msample(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(full_corpus[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m])), \u001b[39m2000000\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m full_corpus_used \u001b[39m=\u001b[39m [tokenize_jp(item[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(full_corpus_used)]\n",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m, in \u001b[0;36mtokenize_jp\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_jp\u001b[39m(text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     tokens \u001b[39m=\u001b[39m mecab\u001b[39m.\u001b[39;49mparse(text)\u001b[39m.\u001b[39msplit()\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 153028/2000000 [02:56<41:38, 739.10it/s]"
     ]
    }
   ],
   "source": [
    "# all corpus texts\n",
    "CORPUS_PATH = f\"{OUTPUT_DIR}/miracl_subset.json\"\n",
    "if os.path.exists(CORPUS_PATH):\n",
    "    with open(CORPUS_PATH, \"r\") as f:\n",
    "        miracle_subset = json.load(f)\n",
    "else:\n",
    "    full_corpus = datasets.load_dataset(\"miracl/miracl-corpus\", \"ja\")\n",
    "    miracl_subset = full_corpus[\"train\"].select(random.sample(range(len(full_corpus[\"train\"])), 2000000))\n",
    "    miracl_subset = [tokenize_jp(item[\"text\"]) for item in tqdm.tqdm(miracl_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "amagasaki_idf_recalls = []\n",
    "miracl_idf_recalls = []\n",
    "\n",
    "for _ in tqdm.tqdm(range(1)):\n",
    "    corpus = full_corpus[\"train\"].select(random.sample(range(len(full_corpus[\"train\"])), 100000))\n",
    "    corpus_texts = [tokenize_jp(item[\"text\"]) for item in corpus]\n",
    "    vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "    vectorizer.fit(corpus_texts)\n",
    "\n",
    "    recall_at_5 = calc_recall(vectorizer, n=5)\n",
    "    miracl_idf_recalls.append(recall_at_5)\n",
    "\n",
    "# 平均と標準偏差を計算し表示\n",
    "miracl_idf_recall_mean = np.mean(miracl_idf_recalls)\n",
    "miracl_idf_recall_std = np.std(miracl_idf_recalls)\n",
    "print(f\"miracl_idf_recall: {miracl_idf_recall_mean} ± {miracl_idf_recall_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = full_corpus[\"train\"].shuffle(seed=random.randint(0, 1000)).select(range(1000))\n",
    "corpus = full_corpus[\"train\"].select(random.sample(range(len(full_corpus[\"train\"])), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(query_size, corpus_size):\n",
    "    # ランダムにクエリを選択\n",
    "    \n",
    "\n",
    "    positive_corpus_json = []\n",
    "    query_texts = []\n",
    "    done_docids = set()\n",
    "    for item in ds:\n",
    "        query_texts.append(tokenize_jp(item[\"query\"]))\n",
    "        for pp in item[\"positive_passages\"]:\n",
    "            if pp[\"docid\"] in done_docids:\n",
    "                continue\n",
    "            positive_corpus_json.append({\n",
    "                \"text\": tokenize_jp(pp[\"text\"]),\n",
    "                \"docid\": pp[\"docid\"]\n",
    "            })\n",
    "    positive_docids = set([x[\"docid\"] for x in positive_corpus_json])\n",
    "\n",
    "    # ランダムにコーパスを選択\n",
    "    max_corpus_size = corpus_size*2 + len(positive_docids)\n",
    "    corpus_without_positive = full_corpus[\"train\"].select(random.sample(range(len(full_corpus[\"train\"])), max_corpus_size)).filter(lambda x: x[\"docid\"] not in positive_docids)\n",
    "    corpus_without_positive_json = [{\"docid\": doc[\"docid\"], \"text\": tokenize_jp(doc[\"text\"])} for doc in corpus_without_positive]\n",
    "\n",
    "    train_corpus = corpus_without_positive_json[:corpus_size]\n",
    "    test_corpus = positive_corpus_json + corpus_without_positive_json[corpus_size:corpus_size*2-len(positive_corpus_json)]\n",
    "    assert len(test_corpus) == corpus_size\n",
    "    assert len(train_corpus) == corpus_size\n",
    "    return ds, query_texts, train_corpus, test_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846eeb7cd6a1442884129868d202735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6953614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "item = full_ds[0]\n",
    "docids = set([x[\"docid\"] for x in item[\"positive_passages\"]])\n",
    "docs = full_corpus[\"train\"].filter(lambda x: x[\"docid\"] in docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cac2246b5e7406ea9d085c4d18b5673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d0976b9c754be38cd63252a7ad6f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:02<00:11,  1.49s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f25d574eebe47b4923bf3d771d2d829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:10,  1.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d53e730f3eb4558991d22ea8874e301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:05<00:08,  1.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ea5158f39d4976af358d8abf30b74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:06<00:06,  1.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119449da96f549d68aaa35cd57dc5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:08<00:05,  1.49s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3015dc3bc8449508896390ba42068da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:10<00:04,  1.49s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e8a9db8b3743aa9c5111ecda7f1c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:12<00:03,  1.65s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947128dab67a4b548dd2054f8d64bdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:13<00:01,  1.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4831940faadd42f393db3f68574bdd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train recall@5: 0.8698722943722943 ± 0.04472819471100963\n",
      "test recall@5: 0.863038961038961 ± 0.03786960883767521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CORPUS_SIZE = 500\n",
    "QUERY_SIZE = 30\n",
    "train_recalls = []\n",
    "test_recalls = []\n",
    "for _ in tqdm.tqdm(range(10)):\n",
    "    ds, query_texts, train_corpus, test_corpus = get_corpus(QUERY_SIZE, CORPUS_SIZE)\n",
    "    \n",
    "    def calc_result(test_corpus, vectorizer, n = 5):\n",
    "        global query_texts, ds\n",
    "        \n",
    "        test_matrix = vectorizer.transform([doc[\"text\"] for doc in test_corpus])\n",
    "        query_matrix = vectorizer.transform(query_texts)\n",
    "\n",
    "        # 類似度行列を計算し、queryのdocidのランクを取得\n",
    "        similarity_matrix = cosine_similarity(query_matrix, test_matrix)\n",
    "        ranking_matrix = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "\n",
    "        test_docid2indice = {item[\"docid\"]: i for i, item in enumerate(test_corpus)}    \n",
    "\n",
    "        query_result = []\n",
    "        for item, ranking in zip(ds, ranking_matrix):\n",
    "            # rankingの何番目にdocidがあるかを取得\n",
    "            docids = [pp[\"docid\"] for pp in item[\"positive_passages\"]]\n",
    "            docid_indices = [test_docid2indice[docid] for docid in docids if docid in test_docid2indice]\n",
    "            ranks = [list(ranking).index(docid_index) for docid_index in docid_indices]\n",
    "            query_result.append({\n",
    "                \"query_id\": item[\"query_id\"],\n",
    "                \"ranks\": ranks\n",
    "            })\n",
    "        \n",
    "        # recall@nを計算\n",
    "        recall_at_n = np.mean([np.mean([1 if rank < n else 0 for rank in item[\"ranks\"]]) for item in query_result])\n",
    "        return recall_at_n\n",
    "    full_vocabulary = set()\n",
    "\n",
    "    for query_text in query_texts:\n",
    "        full_vocabulary.update(query_text)\n",
    "    for doc in train_corpus + test_corpus:\n",
    "        full_vocabulary.update(doc[\"text\"])\n",
    "\n",
    "    train_vectorizer = TfidfVectorizer(analyzer=lambda x: x, vocabulary=full_vocabulary, min_df=10)\n",
    "    train_vectorizer.fit([x[\"text\"] for x in train_corpus])\n",
    "\n",
    "    test_vectorizer = TfidfVectorizer(analyzer=lambda x: x, vocabulary=full_vocabulary, min_df=10)\n",
    "    test_vectorizer.fit([x[\"text\"] for x in test_corpus])\n",
    "    \n",
    "    train_recall = calc_result(test_corpus, train_vectorizer, 5)\n",
    "    test_recall = calc_result(test_corpus, test_vectorizer, 5)\n",
    "\n",
    "    train_recalls.append(train_recall)\n",
    "    test_recalls.append(test_recall)\n",
    "\n",
    "# 平均と標準偏差を計算\n",
    "train_recall_mean = np.mean(train_recalls)\n",
    "train_recall_std = np.std(train_recalls)\n",
    "test_recall_mean = np.mean(test_recalls)\n",
    "test_recall_std = np.std(test_recalls)\n",
    "# 表示\n",
    "print(f\"train recall@5: {train_recall_mean} ± {train_recall_std}\")  \n",
    "print(f\"test recall@5: {test_recall_mean} ± {test_recall_std}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t検定: t=1.6999624307531316, p=0.12335490730783027\n"
     ]
    }
   ],
   "source": [
    "# recallsを対応ありのt検定で比較\n",
    "from scipy import stats\n",
    "t, p = stats.ttest_rel(train_recalls, test_recalls)\n",
    "print(f\"t検定: t={t}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['サッカー', 'の', '発祥', '地', 'は', 'どこ']\n",
      "サッカー\n",
      "5.830311739964975\n",
      "5.270695952029551\n",
      "発祥\n",
      "7.2166061010848646\n",
      "5.607168188650765\n"
     ]
    }
   ],
   "source": [
    "print(query_texts[0])\n",
    "id = train_vectorizer.vocabulary_[\"サッカー\"]\n",
    "print(\"サッカー\")\n",
    "print(train_vectorizer.idf_[id])\n",
    "print(test_vectorizer.idf_[id])\n",
    "id = train_vectorizer.vocabulary_[\"発祥\"]\n",
    "print(\"発祥\")\n",
    "print(train_vectorizer.idf_[id])\n",
    "print(test_vectorizer.idf_[id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
